\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{color}
\lstset{language=python,
                breaklines=true,
                basicstyle=\ttfamily\scriptsize,
                numbers=left,
                language=C,
                keywordstyle=\color{blue}\ttfamily\bfseries,
                stringstyle=\color{red}\ttfamily,
                commentstyle=\color{commentgreen}\ttfamily,
                frame=single,
                morecomment=[l][\color{magenta}]{\#}
}

\title{Artificial Intelligence Miniproject\\Perceptron learning}
\author{Jannick Drews}

\begin{document}

\maketitle

\section{Dataset \& modifications}
The dataset used in this project is from the Kaggle website:
\url{
		https://www.kaggle.com/spscientist/students-performance-in-exams
	}\\
I used this dataset, converted it into 4 different categories which I feed into my network. These 4 categories are;
\begin{itemize}
	\item Gender
	\item Test preparation
	\item Parents educational degree ($>$ associates degree)
	\item If the student ate lunch
\end{itemize}
The student would need a score $> 65$ out of a 100 to pass.

\section{code}
\lstinputlisting{NNmini.py}
Up until line 60, the code is merely adjusting the needed data for the AI to handle. This includes reading from the CSV file and appending it properly to the needed input data. Getting tthe data for whether the student have had lunch, parental education, if they prepared and their gender.\\

Two functions are declared at line 62 and 65. These are the sigmoid and sigmoid derivative functions used for the feedforward and backpropagation.\\
The weights are initialized randomly as well as the bias at lines 68 and 69.\\
Lines 72 to 88 contains the learning for the network. This is done in a loop to 2500 for learning with epochs.\\
Lines 76 to 77 contains the feedforward instructions. With taking the sum of the dot products of the set with its weights, with the bias added afterwards. This is then put into the sigmoid function for the output.\\
Lines 80 to 84 contains the backpropagation. Finding the error margin compared to the real result to see how far off the guess was. Lines 85 and 88 then adjusts the bias and weights of the network before we try again in the next loop.


\end{document}
